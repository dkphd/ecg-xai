{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import six\n",
    "\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from radiomics import featureextractor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_features(scan, row_dict = {}):\n",
    "    extractor = featureextractor.RadiomicsFeatureExtractor(\"config.yml\") \n",
    "    mask = np.ones_like(scan)\n",
    "    mask[0,0] = 0\n",
    "    result = extractor.execute(sitk.GetImageFromArray(scan), sitk.GetImageFromArray(mask) ,voxelBased=False)\n",
    "    extractor.enableAllImageTypes()\n",
    "    names = []\n",
    "    vals = []\n",
    "    for k,v in row_dict.items():\n",
    "        names.append(k)\n",
    "        vals.append(v)\n",
    "    for key, val in six.iteritems(result):\n",
    "        if 'diagn' not in key:\n",
    "            names.append(key)\n",
    "            vals.append(val)\n",
    "    return pd.Series(data=vals, index = names)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Starting\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "def split_into_tiles(image, tile_size=30):\n",
    "    w, h = 512, 1024\n",
    "    # Adjusting the height and width to be divisible by tile_size\n",
    "    h = (h // tile_size) * tile_size\n",
    "    w = (w // tile_size) * tile_size\n",
    "    \n",
    "    tiles = np.zeros((h // tile_size, w // tile_size), dtype=int)  # Ensuring binary tiles\n",
    "    \n",
    "    for i in range(0, h, tile_size):\n",
    "        for j in range(0, w, tile_size):\n",
    "            # Take the tile and check if there is any '1' in the mask (binary representation)\n",
    "            tile = image[i:i+tile_size, j:j+tile_size]\n",
    "            tiles[i // tile_size, j // tile_size] = np.any(tile)  # Mark tile as 1 if there's any non-zero pixel\n",
    "    \n",
    "    return tiles\n",
    "\n",
    "def calculate_iou(df):\n",
    "    grouped = df.groupby([\"model_name\", \"npz_name\"])\n",
    "    results = []\n",
    "\n",
    "    for (model_name, npz_name), group in grouped:\n",
    "        clear_image_mask = None\n",
    "        other_masks = []\n",
    "\n",
    "        # Collect masks for this group\n",
    "        for _, row in group.iterrows():\n",
    "            data = np.load(row[\"full_path\"], allow_pickle=True)\n",
    "            if \"lime\" in data:\n",
    "                transformed_mask = split_into_tiles(data[\"lime\"])\n",
    "                if row[\"mode\"] == \"clear_images\":\n",
    "                    clear_image_mask = transformed_mask\n",
    "                else:\n",
    "                    other_masks.append((row[\"mode\"], transformed_mask))\n",
    "\n",
    "        # Initialize dictionary for storing IoUs\n",
    "        iou_dict = {}\n",
    "\n",
    "        # Calculate IoU for each non-\"clear_image\" mask with the \"clear_image\" mask\n",
    "        if clear_image_mask is not None:\n",
    "            for mode, mask in other_masks:\n",
    "                intersection = np.logical_and(clear_image_mask, mask).sum()\n",
    "                union = np.logical_or(clear_image_mask, mask).sum()\n",
    "                iou = intersection / union if union > 0 else 0\n",
    "                iou_dict[f\"iou_{mode}\"] = iou\n",
    "        else:\n",
    "            iou_dict[\"error\"] = \"No clear_image mask found for this group\"\n",
    "\n",
    "        # Append result for the current group\n",
    "        result_entry = {\n",
    "            \"model_name\": model_name,\n",
    "            \"npz_name\": npz_name\n",
    "        }\n",
    "        result_entry.update(iou_dict)\n",
    "        results.append(result_entry)\n",
    "\n",
    "    return pd.DataFrame(results)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "\n",
    "def extract_npz_paths(root_dir):\n",
    "    data = []\n",
    "    \n",
    "    # Collect data from the directory\n",
    "    for dirpath, _, filenames in os.walk(root_dir):\n",
    "        for file in filenames:\n",
    "            if file.endswith(\".npz\"):\n",
    "                full_path = os.path.join(dirpath, file)\n",
    "                parts = full_path.split(os.sep)\n",
    "                \n",
    "                if len(parts) >= 3:  # Ensure there are enough parts\n",
    "                    entry = {\n",
    "                        \"full_path\": full_path,\n",
    "                        \"model_name\": parts[-3],\n",
    "                        \"mode\": parts[-2],\n",
    "                        \"npz_name\": parts[-1]\n",
    "                    }\n",
    "                    data.append(entry)\n",
    "\n",
    "    # Convert to DataFrame\n",
    "    df = pd.DataFrame(data)\n",
    "    \n",
    "    # Group by 'model_name' and 'mode' and select the first 10 samples from each group\n",
    "    df_grouped = df.groupby(['model_name', 'mode']).head(100).reset_index(drop=True)\n",
    "    \n",
    "    # Unwinding (resetting index ensures it's fully flattened)\n",
    "    df_flat = df_grouped.reset_index(drop=True)\n",
    "\n",
    "    return df_flat\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir_df = extract_npz_paths(\"./explanation3\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir_df.to_csv(\"paths.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import argparse\n",
    "import six\n",
    "\n",
    "import SimpleITK as sitk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from radiomics import featureextractor\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def calculate_features(scan, row_dict = {}):\n",
    "    extractor = featureextractor.RadiomicsFeatureExtractor(\"config.yml\") \n",
    "    mask = np.ones_like(scan)\n",
    "    mask[0,0] = 0\n",
    "    result = extractor.execute(sitk.GetImageFromArray(scan), sitk.GetImageFromArray(mask) ,voxelBased=False)\n",
    "    extractor.enableAllImageTypes()\n",
    "    names = []\n",
    "    vals = []\n",
    "    for k,v in row_dict.items():\n",
    "        names.append(k)\n",
    "        vals.append(v)\n",
    "    for key, val in six.iteritems(result):\n",
    "        if 'diagn' not in key:\n",
    "            names.append(key)\n",
    "            vals.append(val)\n",
    "    return pd.Series(data=vals, index = names)\n",
    "\n",
    "\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    args = {\n",
    "        \"from\": 0,\n",
    "        \"to\": len(root_dir_df)\n",
    "    }\n",
    "    df = pd.read_csv(\"./paths.csv\")\n",
    "    print(len(df))\n",
    "    result = []\n",
    "    for idx, row in df.iloc[args['from']:args['to']].iterrows():\n",
    "        print(idx)\n",
    "        row_dict = dict(row)\n",
    "        array = np.load(row[\"full_path\"], allow_pickle=True)\n",
    "        lime = (array['lime'] + 1) * 127.5\n",
    "        lime = array['lime']\n",
    "        features = calculate_features(lime, row_dict)\n",
    "        result.append(features)\n",
    "        pd.DataFrame(result).to_csv(f\"{args['from']}-{args['to']}_no_scaling.csv\")\n",
    "\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating IoU statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "root_dir_df.groupby(['model_name', 'mode']).first()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_df = calculate_iou(root_dir_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_df['avg_iou'] = iou_df[['iou_full', 'iou_no_header', 'iou_text', 'iou_wrinkles']].mean(axis=1)\n",
    "\n",
    "# Sort the DataFrame by 'avg_iou' in descending order\n",
    "sorted_iou_scores_df = iou_df.sort_values(by='avg_iou', ascending=False)\n",
    "\n",
    "# Display the result\n",
    "sorted_iou_scores_df[['npz_name', 'model_name', 'avg_iou']].to_csv(\"test111.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sorted_iou_scores_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iou_df.drop(columns=['npz_name']).groupby(['model_name']).mean().round(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Replace model names with specified new names\n",
    "iou_df['model_name'] = iou_df['model_name'].replace({\n",
    "    'phd-dk_ptb_xl_training_xai_fixed_model-htdatole_v0': \"Model 2\",\n",
    "    'phd-dk_ptb_xl_training_xai_fixed_model-jr52wzjb_v0': \"Model 5\",\n",
    "    'phd-dk_ptb_xl_training_xai_fixed_model-z2gk30z8_v0': \"Model 1\"\n",
    "})\n",
    "\n",
    "# Extract number from 'npz_name' and remove leading zeros\n",
    "def extract_number_from_npz_name(name):\n",
    "    match = re.search(r'(\\d+)_', name)\n",
    "    if match:\n",
    "        return str(int(match.group(1)))  # Convert to int to remove leading zeros\n",
    "    return name  # Fallback to the original name if no match\n",
    "\n",
    "iou_df['npz_name'] = iou_df['npz_name'].apply(extract_number_from_npz_name)\n",
    "\n",
    "# Selector for method: Choose between 'best_worst' and 'percentiles'\n",
    "method = 'percentiles'  # Change to 'best_worst' or 'percentiles'\n",
    "\n",
    "result_dfs = []\n",
    "\n",
    "for model in iou_df['model_name'].unique():\n",
    "    # Filter for the current model\n",
    "    filtered_df = iou_df[iou_df['model_name'] == model]\n",
    "    \n",
    "    if method == 'best_worst':\n",
    "        # Sort by the metric (replace 'avg_iou' with your actual metric column)\n",
    "        sorted_df = filtered_df.sort_values(by='avg_iou', ascending=False)\n",
    "        \n",
    "        # Select top 3 and bottom 3\n",
    "        best_examples = sorted_df.head(3)\n",
    "        worst_examples = sorted_df.tail(3)\n",
    "        \n",
    "        # Combine best and worst examples\n",
    "        combined_df = pd.concat([best_examples, worst_examples])\n",
    "\n",
    "    elif method == 'percentiles':\n",
    "        # Calculate the 75th and 25th percentiles\n",
    "        p75 = filtered_df['avg_iou'].quantile(0.75)\n",
    "        p25 = filtered_df['avg_iou'].quantile(0.25)\n",
    "        \n",
    "        # Select examples closest to 75th and 25th percentiles\n",
    "        p75_examples = filtered_df.iloc[(filtered_df['avg_iou'] - p75).abs().argsort()[:1]]\n",
    "        p25_examples = filtered_df.iloc[(filtered_df['avg_iou'] - p25).abs().argsort()[:1]]\n",
    "        \n",
    "        # Combine percentile examples\n",
    "        combined_df = pd.concat([p75_examples, p25_examples])\n",
    "\n",
    "    # Add to list of results\n",
    "    result_dfs.append(combined_df)\n",
    "\n",
    "# Combine all results for each model into one DataFrame\n",
    "final_df = pd.concat(result_dfs)\n",
    "\n",
    "# Round all numeric columns to 3 significant digits\n",
    "def round_to_sig_digits(x, sig_digits=4):\n",
    "    if isinstance(x, (int, float)):\n",
    "        return f\"{x:.{sig_digits-1}g}\"  # Format to significant digits\n",
    "    return x\n",
    "\n",
    "final_df = final_df.applymap(round_to_sig_digits)\n",
    "\n",
    "# Save the combined results to LaTeX\n",
    "final_df.to_latex(\"median_examples_all_models3.lx\", index=False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculating pyradiomic statistics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "# Specify the directory to search\n",
    "directory = \".\"\n",
    "\n",
    "# Define the pattern (e.g., XX-YY.csv)\n",
    "pattern =  re.compile(r'^\\d+-\\d+_no_scaling\\.csv$')\n",
    "paths = []\n",
    "# Walk through the directory and match files\n",
    "for root, _, files in os.walk(directory):\n",
    "    for file in files:\n",
    "        if pattern.match(file) and \"old\" not in os.path.join(root, file):\n",
    "            paths.append(os.path.join(root, file))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataframes = [pd.read_csv(path) for path in paths]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df =  pd.concat(dataframes, ignore_index=True)\n",
    "df = df.drop(columns=['Unnamed: 0'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dython.nominal import associations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.index = df.full_path\n",
    "feature_columns = df.select_dtypes(include=['number']).columns\n",
    "features = df[feature_columns]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(feature_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import silhouette_score\n",
    "\n",
    "# Step 1: Standardize the features (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "standardized_features = scaler.fit_transform(features)\n",
    "\n",
    "# Step 2: Apply PCA to reduce dimensionality while retaining 95% of the variance\n",
    "pca = PCA(n_components=0.99)\n",
    "pca_features = pca.fit_transform(standardized_features)\n",
    "\n",
    "# Step 3: Create a DataFrame for the PCA-transformed data\n",
    "pca_df = pd.DataFrame(pca_features, columns=[f\"PC{i+1}\" for i in range(pca_features.shape[1])])\n",
    "pca_df.index = df.full_path\n",
    "\n",
    "# Step 4: Optionally, merge the PCA results back with the original data\n",
    "combined_with_pca = pd.concat([df, pca_df], axis=1)\n",
    "\n",
    "# Step 5: Find the optimal number of clusters using the Elbow Method and Silhouette Score\n",
    "def find_optimal_k(data, mode, max_clusters=10):\n",
    "    wcss = []  # Within-cluster sum of squares\n",
    "    silhouette_scores = []\n",
    "\n",
    "    for k in range(2, max_clusters + 1):\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        clusters = kmeans.fit_predict(data)\n",
    "        wcss.append(kmeans.inertia_)\n",
    "\n",
    "        # Calculate silhouette score\n",
    "        score = silhouette_score(data, mode)\n",
    "        silhouette_scores.append(score)\n",
    "\n",
    "    # Print the optimal k based on the highest silhouette score\n",
    "    print(silhouette_scores)\n",
    "    optimal_k = silhouette_scores.index(max(silhouette_scores)) + 2\n",
    "    print(f\"Optimal number of clusters (based on Silhouette Score): {optimal_k}\")\n",
    "\n",
    "    return optimal_k\n",
    "\n",
    "# Step 6: Find the optimal number of clusters\n",
    "optimal_k = find_optimal_k(pca_features, combined_with_pca['model_name'],max_clusters=10)\n",
    "\n",
    "# Step 7: Apply KMeans with the optimal number of clusters\n",
    "kmeans = KMeans(n_clusters=optimal_k, random_state=42)\n",
    "clusters = kmeans.fit_predict(pca_features)\n",
    "combined_with_pca['cluster'] = clusters\n",
    "\n",
    "# Step 8: Optional - Perform associations or further analysis on the clusters\n",
    "r = associations(combined_with_pca[['mode', 'cluster']])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Veryfing the most important pca features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_pcs= pca.components_.shape[0]\n",
    "\n",
    "# get the index of the most important feature on EACH component\n",
    "# LIST COMPREHENSION HERE\n",
    "most_important = [np.abs(pca.components_[i]).argmax() for i in range(n_pcs)]\n",
    "most_important_names = [features.columns[most_important[i]] for i in range(n_pcs)]\n",
    "most_important_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "def accuracy_per_class(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Calculate classification accuracy per class.\n",
    "    \n",
    "    Parameters:\n",
    "    -----------\n",
    "    y_true : array-like\n",
    "        Ground truth labels\n",
    "    y_pred : array-like\n",
    "        Predicted labels\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    dict : Dictionary mapping class labels to their respective accuracy scores\n",
    "    \"\"\"\n",
    "    # Create confusion matrix\n",
    "    cm = confusion_matrix(y_true, y_pred)\n",
    "    \n",
    "    # Get the list of unique classes\n",
    "    classes = np.unique(np.concatenate([y_true, y_pred]))\n",
    "    \n",
    "    # Calculate accuracy for each class\n",
    "    accuracies = {}\n",
    "    for i, class_label in enumerate(classes):\n",
    "        # True positives for this class\n",
    "        tp = cm[i, i]\n",
    "        \n",
    "        # Total samples of this class in ground truth\n",
    "        total = np.sum(cm[i, :])\n",
    "        \n",
    "        # Calculate accuracy\n",
    "        accuracy = tp / total if total > 0 else 0\n",
    "        accuracies[class_label] = accuracy\n",
    "    \n",
    "    return accuracies"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Training simple classifier to perform model analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Standardize the features\n",
    "scaler = StandardScaler()\n",
    "standardized_features = scaler.fit_transform(features)\n",
    "\n",
    "# Step 2: Apply PCA\n",
    "pca = PCA(n_components=0.99)\n",
    "pca_features = pca.fit_transform(standardized_features)\n",
    "\n",
    "# Step 3: Create a DataFrame for the PCA-transformed data\n",
    "pca_df = pd.DataFrame(pca_features, columns=[f\"PC{i+1}\" for i in range(pca_features.shape[1])])\n",
    "pca_df.index = df.full_path\n",
    "\n",
    "# Merge the PCA results with the original DataFrame\n",
    "combined_with_pca = pd.concat([df, pca_df], axis=1)\n",
    "\n",
    "# Step 4: Add KMeans clustering\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "clusters = kmeans.fit_predict(pca_features)\n",
    "combined_with_pca['cluster'] = clusters\n",
    "\n",
    "# Step 5: Prepare data for classification\n",
    "X = pca_df  # PCA-transformed features\n",
    "combined_with_pca['mode'] = combined_with_pca['mode'].replace({'clear_images': \"Base Image\",\n",
    " \"full\": \"Discoloration\",\n",
    " \"no_header\": \"No patient Data\",\n",
    " \"text\": \"Handwriting\",\n",
    " \"wrinkles\": \"Wrinkles\"})\n",
    "y = combined_with_pca['mode']  # Target variable\n",
    "\n",
    "# Step 6: Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Step 7: Train a classification model\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Step 8: Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Step 9: Evaluate the classification model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Step 10: Print evaluation metrics\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Extract class names (ensures labels match correctly)\n",
    "class_names = y.unique()  # Extract unique class labels for display\n",
    "\n",
    "# Convert confusion matrix to a DataFrame for better readability\n",
    "cm_df = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "\n",
    "# Print the confusion matrix as a table\n",
    "print(\"Confusion Matrix:\")\n",
    "cm_df\n",
    "print(accuracy_per_class(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "\n",
    "# Step 1: Standardize the features\n",
    "scaler = StandardScaler()\n",
    "standardized_features = scaler.fit_transform(features)\n",
    "\n",
    "# Step 2: Apply PCA\n",
    "pca = PCA(n_components=0.99)\n",
    "pca_features = pca.fit_transform(standardized_features)\n",
    "\n",
    "# Step 3: Create a DataFrame for the PCA-transformed data\n",
    "pca_df = pd.DataFrame(pca_features, columns=[f\"PC{i+1}\" for i in range(pca_features.shape[1])])\n",
    "pca_df.index = df.full_path\n",
    "\n",
    "# Merge the PCA results with the original DataFrame\n",
    "combined_with_pca = pd.concat([df, pca_df], axis=1)\n",
    "\n",
    "# Step 4: Add KMeans clustering\n",
    "kmeans = KMeans(n_clusters=5, random_state=42)\n",
    "clusters = kmeans.fit_predict(pca_features)\n",
    "combined_with_pca['cluster'] = clusters\n",
    "\n",
    "# Step 5: Prepare data for classification\n",
    "X = pca_df  # PCA-transformed features\n",
    "combined_with_pca['model_name'] = combined_with_pca['model_name'].replace({'phd-dk_ptb_xl_training_xai_fixed_model-htdatole_v0': \"Model 2\",\n",
    " \"phd-dk_ptb_xl_training_xai_fixed_model-jr52wzjb_v0\": \"Model 5\",\n",
    " \"phd-dk_ptb_xl_training_xai_fixed_model-z2gk30z8_v0\": \"Model 1\"\n",
    "})\n",
    "y = combined_with_pca['model_name']  # Target variable\n",
    "\n",
    "# Step 6: Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "# Step 7: Train a classification model\n",
    "clf = RandomForestClassifier(random_state=42)\n",
    "clf.fit(X_train, y_train)\n",
    "\n",
    "# Step 8: Make predictions\n",
    "y_pred = clf.predict(X_test)\n",
    "\n",
    "# Step 9: Evaluate the classification model\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "precision = precision_score(y_test, y_pred, average='weighted')\n",
    "recall = recall_score(y_test, y_pred, average='weighted')\n",
    "f1 = f1_score(y_test, y_pred, average='weighted')\n",
    "\n",
    "# Step 10: Print evaluation metrics\n",
    "print(\"Classification Report:\")\n",
    "print(classification_report(y_test, y_pred))\n",
    "\n",
    "print(f\"Accuracy: {accuracy:.4f}\")\n",
    "print(f\"Precision: {precision:.4f}\")\n",
    "print(f\"Recall: {recall:.4f}\")\n",
    "print(f\"F1 Score: {f1:.4f}\")\n",
    "\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, classification_report, confusion_matrix\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "\n",
    "# Extract class names (ensures labels match correctly)\n",
    "class_names = y.unique()  # Extract unique class labels for display\n",
    "\n",
    "# Convert confusion matrix to a DataFrame for better readability\n",
    "cm_df = pd.DataFrame(cm, index=class_names, columns=class_names)\n",
    "\n",
    "# Print the confusion matrix as a table\n",
    "print(\"Confusion Matrix:\")\n",
    "cm_df\n",
    "print(accuracy_per_class(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(pca_features[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca_features.shape[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "explained_variance = pca.explained_variance_ratio_\n",
    "\n",
    "# Step 4: Get the principal components\n",
    "components = pca.components_\n",
    "\n",
    "# Step 5: Display the most important features for each principal component\n",
    "# Here we take the absolute values to see the most influential features\n",
    "importance = pd.DataFrame(components, columns=feature_columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importance = pd.DataFrame(components, columns=feature_columns)\n",
    "\n",
    "# Sum the absolute values across all components to get the total importance\n",
    "total_importance = importance.abs().sum(axis=0) / len(importance)\n",
    "\n",
    "# Step 5: Sort the features by their total importance\n",
    "sorted_importance = total_importance.sort_values(ascending=False)\n",
    "\n",
    "# Display the most important features\n",
    "print(sorted_importance.head(8))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.components_"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Performing clusteristion of model used and method of augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.rcParams['font.family'] = 'Times New Roman'\n",
    "plt.rcParams[\"font.size\"] = 18"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Step 1: Standardize the features (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "standardized_features = scaler.fit_transform(features)\n",
    "\n",
    "# Step 2: Apply PCA\n",
    "pca = PCA(n_components=0.99)\n",
    "pca_features = pca.fit_transform(standardized_features)\n",
    "\n",
    "# Step 3: Create a DataFrame for the PCA-transformed data\n",
    "pca_df = pd.DataFrame(pca_features, columns=[f\"PC{i+1}\" for i in range(pca_features.shape[1])])\n",
    "pca_df.index = df.full_path\n",
    "\n",
    "# Optionally, you can merge the PCA results back to the original DataFrame\n",
    "combined_with_pca = pd.concat([df, pca_df], axis=1)\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tsne_features = tsne.fit_transform(pca_features)  # Apply t-SNE on PCA features\n",
    "\n",
    "# Add the t-SNE results to the DataFrame\n",
    "combined_with_pca['tSNE-1'] = tsne_features[:, 0]\n",
    "combined_with_pca['tSNE-2'] = tsne_features[:, 1]\n",
    "\n",
    "# Step 4: Apply KMeans clustering\n",
    "kmeans = KMeans(n_clusters=6, random_state=42)\n",
    "clusters = kmeans.fit_predict(tsne_features)\n",
    "combined_with_pca['cluster'] = clusters\n",
    "\n",
    "# Step 5: Apply t-SNE for 2D visualization\n",
    "combined_with_pca['model_name'] = combined_with_pca['model_name'].replace({'phd-dk_ptb_xl_training_xai_fixed_model-htdatole_v0': \"Model 2\",\n",
    " \"phd-dk_ptb_xl_training_xai_fixed_model-jr52wzjb_v0\": \"Model 5\",\n",
    " \"phd-dk_ptb_xl_training_xai_fixed_model-z2gk30z8_v0\": \"Model 1\"\n",
    "})\n",
    "# Step 6: Visualize the clusters using a scatter plot\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(x='tSNE-1', y='tSNE-2', hue='model_name', palette='Set1', data=combined_with_pca, s=100, marker='o', legend='full')\n",
    "\n",
    "# Customize plot\n",
    "plt.xlabel(\"t-SNE component 1\")\n",
    "plt.ylabel(\"t-SNE component 2\")\n",
    "plt.legend(title=\"Model\", loc='upper left')\n",
    "# plt.show()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"model_name.pdf\", dpi=1000, format=\"pdf\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Step 1: Standardize the features (mean=0, std=1)\n",
    "scaler = StandardScaler()\n",
    "standardized_features = scaler.fit_transform(features)\n",
    "\n",
    "# Step 2: Apply PCA\n",
    "pca = PCA(n_components=0.99)\n",
    "pca_features = pca.fit_transform(standardized_features)\n",
    "\n",
    "# Step 3: Create a DataFrame for the PCA-transformed data\n",
    "pca_df = pd.DataFrame(pca_features, columns=[f\"PC{i+1}\" for i in range(pca_features.shape[1])])\n",
    "pca_df.index = df.full_path\n",
    "\n",
    "# Optionally, you can merge the PCA results back to the original DataFrame\n",
    "combined_with_pca = pd.concat([df, pca_df], axis=1)\n",
    "tsne = TSNE(n_components=2, random_state=42)\n",
    "tsne_features = tsne.fit_transform(pca_features)  # Apply t-SNE on PCA features\n",
    "\n",
    "# Add the t-SNE results to the DataFrame\n",
    "combined_with_pca['tSNE-1'] = tsne_features[:, 0]\n",
    "combined_with_pca['tSNE-2'] = tsne_features[:, 1]\n",
    "\n",
    "# Step 4: Apply KMeans clustering\n",
    "kmeans = KMeans(n_clusters=6, random_state=42)\n",
    "clusters = kmeans.fit_predict(tsne_features)\n",
    "combined_with_pca['cluster'] = clusters\n",
    "\n",
    "# Step 5: Apply t-SNE for 2D visualization\n",
    "\n",
    "combined_with_pca['mode'] = combined_with_pca['mode'].replace({'clear_images': \"Base Image\",\n",
    " \"full\": \"Discoloration\",\n",
    " \"no_header\": \"No patient Data\",\n",
    " \"text\": \"Handwriting\",\n",
    " \"wrinkles\": \"Wrinkles\"})\n",
    "# Step 6: Visualize the clusters using a scatter plot\n",
    "plt.figure(figsize=(8, 8))\n",
    "sns.scatterplot(x='tSNE-1', y='tSNE-2', hue='mode', palette='Set1', data=combined_with_pca, s=100, marker='o', legend='full')\n",
    "\n",
    "# Customize plot\n",
    "plt.xlabel(\"t-SNE component 1\")\n",
    "plt.ylabel(\"t-SNE component 2\")\n",
    "plt.legend(title=\"Modifications\", loc='upper left')\n",
    "# plt.show()\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"mode.pdf\", dpi=1000, format=\"pdf\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "statistics",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
